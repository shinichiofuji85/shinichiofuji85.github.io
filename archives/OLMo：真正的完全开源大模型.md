OLMo 是由 AI2（Allen Institute for AI）推出的一个完全开源的语言模型项目，其特点是 **100% 开放**，包括预训练数据、模型权重、训练代码等所有核心资源。

## 什么是 OLMo？

OLMo（Open Language Model）不仅开放了其完整的预训练数据——3 万亿 token 的 Dolma 数据集，还提供了训练代码、模型权重、推理代码、训练指标和完整日志等原始数据。这种程度的开放性使研究人员能够完全复现模型训练过程，深入理解模型性能，并根据需要对模型进行微调。

## OLMo 的核心特点

### 1. 完整的预训练数据
OLMo 项目提供了完整的预训练数据——AI2 的 Dolma 数据集。Dolma 是一个包含 3 万亿 token 的开放语料库，涵盖了多种数据源，包括网络页面、代码、社交媒体、STEM 论文、书籍和百科资料等内容。这种开放性允许研究人员：

- 深入理解模型的学习基础。
- 根据特定需求重新训练或调整模型。

### 2. 训练代码和模型权重
OLMo 提供了四种不同变体模型的完整模型权重，每种模型都至少训练到 2 万亿令牌。此外，还开放了训练代码、推理代码、训练指标和日志。这种透明性使研究人员能够：

- 完全复现模型训练过程。
- 评估模型性能。
- 根据需要对模型进行微调。

### 3. 评估工具
OLMo 项目包含了开发过程中使用的评估套件，以及 500 多个模型的检查点（每 1000 步保存一次）。这些工具属于 Catwalk 项目的一部分，研究人员可以使用它们来评估自己的模型或对 OLMo 模型进行进一步分析。

## 模型参数和架构

OLMo 提供了不同规模的模型变体，具体包括：

- **1B（10 亿参数）模型**：16 层，每层 2048 个隐藏单元，16 个注意力头，训练了至少 2 万亿个令牌。
- **7B（70 亿参数）模型**：32 层，每层 4086 个隐藏单元，32 个注意力头，训练了约 2.46 万亿个令牌。
- **65B（650 亿参数）模型**：计划包含 80 层，每层 8192 个隐藏单元，64 个注意力头（文章撰写时仍在训练中）。

这些模型基于 Vaswani 等（2017 年）的解码器仅 Transformer 架构，并进行了以下改进：

- 不使用偏置项，以提高训练稳定性。
- 采用非参数层归一化。
- 使用 SwiGLU 激活函数代替 ReLU。
- 引入旋转位置嵌入（RoPE）。
- 使用修改版的 BPE-based 标记器，以减少个人可识别信息（PII）。

## 性能评估

OLMo 7B 在许多生成和阅读理解任务（如 TruthfulQA）上与 Llama 2 表现相当，但在某些流行的问答任务（如 MMLU 或 Big-bench Hard）上略有不足。通过 AI2 的 Paloma 和可用检查点，研究人员分析了模型预测语言能力与模型规模因素（如训练令牌数）之间的关系。

Paloma 试图通过平等地采样每个领域，更均衡地代表 LLM 使用的多种领域。

## 结语

OLMo 的完全开源框架为研究人员提供了前所未有的自由度和透明度。无论是复现模型训练过程，还是根据特定需求进行微调，OLMo 都是一个强大的工具。

👉 [WildCard | 一分钟注册，轻松订阅海外线上服务](https://bit.ly/bewildcard)